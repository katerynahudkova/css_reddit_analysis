{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Initialization and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read one file for analysis. Files must be manually uploaded to /content/submissions folder.\n",
        "df = pd.read_csv(\"/content/submissions/RS_2020-07.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I check which submissions are with NaN values in \"text\" column to decide if they should be deleted:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = df_clean['text'].isnull()\n",
        "df_clean[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that despite the NaN value in \"text\" column for posts above, there is a title, so there's no need to delete these lines.\n",
        "Next, check for dublicates."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_count = df_clean.duplicated().sum()\n",
        "duplicate_count"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change case in both title and text to lower and get clear author username."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[\"title\"] = df_clean[\"title\"].str.lower()\n",
        "df_clean[\"text\"] = df_clean[\"text\"].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[\"author\"] = df_clean[\"author\"].str.strip()\n",
        "df_clean[\"author\"] = df_clean[\"author\"].str.replace(r\"^u/\", \"\", regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get rid of all extra symbols and add a column with pure subreddit name:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean[\"text\"] = (\n",
        "    df_clean[\"text\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True) # FIXED: keeps spaces between words\n",
        "    .str.replace(r\"[^a-zA-Z0-9\\s'.,!?-]\", \"\", regex=True) # FIXED\n",
        ")\n",
        "df_clean[\"title\"] = (\n",
        "    df_clean[\"title\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True) # FIXED\n",
        "    .str.replace(r\"[^a-zA-Z0-9\\s'.,!?-]\", \"\", regex=True) # FIXED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_pattern = r'(?i)(?:https?://)?(?:www\\.|old\\.|np\\.)?reddit\\.com/r/([^/?#]+)/?'\n",
        "df_clean[\"subreddit\"] = df_clean[\"link\"].astype(str).str.extract(sub_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many \"[deleted]\" values in \"author\" and \"text\" columns, so i double-check is there are any lines with both values absent."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deleted_values = ['deleted', '[deleted]']\n",
        "mask_text = df_clean['text'].isin(deleted_values)\n",
        "mask_title = df_clean['title'].isin(deleted_values)\n",
        "combined_mask = mask_text & mask_title\n",
        "print(df_clean[combined_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Batch Processing and Merging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function with all the cleaning\n",
        "def process_reddit_file(df):\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    df_clean[\"author\"] = df_clean[\"author\"].str.strip().str.replace(r\"^u/\", \"\", regex=True)\n",
        "\n",
        "    df_clean[\"text\"] = (\n",
        "        df_clean[\"text\"]\n",
        "        .astype(str)\n",
        "        .str.lower()\n",
        "        .str.strip()\n",
        "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        .str.replace(r\"[^a-zA-Z0-9\\s'.,!?-]\", \"\", regex=True)\n",
        "    )\n",
        "\n",
        "    df_clean[\"title\"] = (\n",
        "        df_clean[\"title\"]\n",
        "        .astype(str)\n",
        "        .str.lower()\n",
        "        .str.strip()\n",
        "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        .str.replace(r\"[^a-zA-Z0-9\\s'.,!?-]\", \"\", regex=True)\n",
        "    )\n",
        "\n",
        "    sub_pattern = r'(?i)(?:https?://)?(?:www\\.|old\\.|np\\.)?reddit\\.com/r/([^/?#]+)/?'\n",
        "    df_clean[\"subreddit\"] = df_clean[\"link\"].astype(str).str.extract(sub_pattern)\n",
        "\n",
        "\n",
        "    empty_values = ['deleted', '[deleted]', 'nan']\n",
        "    mask_text_empty = df_clean['text'].isin(empty_values)\n",
        "    mask_title_empty = df_clean['title'].isin(empty_values)\n",
        "    empty_both_mask = mask_text_empty & mask_title_empty\n",
        "    \n",
        "    duplicates_mask = df_clean.duplicated(subset=[\"author\", \"title\", \"text\", \"created\"])\n",
        "\n",
        "    final_delete_mask = duplicates_mask | empty_both_mask\n",
        "    \n",
        "    #drop unnecessary column (as requested)\n",
        "    if 'url' in df_clean.columns:\n",
        "        df_clean = df_clean.drop(columns=['url'])\n",
        "\n",
        "    df_processed = df_clean[~final_delete_mask]\n",
        "    \n",
        "    return df_processed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup local paths (no Google Drive required)\n",
        "base_dir = \"/content\"\n",
        "submissions_dir = os.path.join(base_dir, \"submissions\")\n",
        "output_dir = submissions_dir # Save in the same directory\n",
        "\n",
        "# Create the submissions directory if it doesn't exist (needed for batch process to run)\n",
        "os.makedirs(submissions_dir, exist_ok=True)\n",
        "\n",
        "file_pattern = os.path.join(submissions_dir, \"RS_*.csv\")\n",
        "csv_files = glob.glob(file_pattern)\n",
        "\n",
        "print(f\"Found {len(csv_files)} files to process.\")\n",
        "print(\"--- Starting batch processing ---\")\n",
        "\n",
        "for file_path in csv_files:\n",
        "    filename = os.path.basename(file_path)\n",
        "    print(f\"--- Processing: {filename} ---\")\n",
        "    \n",
        "    try:\n",
        "        # Read\n",
        "        df = pd.read_csv(file_path)\n",
        "        initial_rows = len(df)\n",
        "        \n",
        "        # Process using our function\n",
        "        df_clean = process_reddit_file(df)\n",
        "        final_rows = len(df_clean)\n",
        "        \n",
        "        # Save\n",
        "        output_filename = f\"cleaned_{filename}\"\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "        df_clean.to_csv(output_path, index=False)\n",
        "        \n",
        "        print(f\"  Initial: {initial_rows}, Final: {final_rows}, Removed: {initial_rows - final_rows}\")\n",
        "        print(f\"  Saved to: {output_path}\\n\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"*** ERROR processing {filename}: {e} ***\\n\")\n",
        "\n",
        "print(\"--- All processing complete. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally merge all cleaned files together."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting merge process ---\")\n",
        "\n",
        "# 1. Find all cleaned files \n",
        "cleaned_pattern = os.path.join(output_dir, \"cleaned_RS_*.csv\")\n",
        "cleaned_files = glob.glob(cleaned_pattern)\n",
        "\n",
        "print(f\"Found {len(cleaned_files)} cleaned files to merge.\")\n",
        "\n",
        "# 2. Use a generator to read files and concatenate them\n",
        "df_generator = (pd.read_csv(f) for f in cleaned_files)\n",
        "merged_df = pd.concat(df_generator, ignore_index=True)\n",
        "\n",
        "# 3. Define save path (saving in the universal /content directory)\n",
        "merged_filename = \"merged_submissions.csv\"\n",
        "merged_path = os.path.join(base_dir, merged_filename)\n",
        "\n",
        "# 4. Save the final merged file\n",
        "merged_df.to_csv(merged_path, index=False)\n",
        "\n",
        "print(\"--- Merge complete! ---\")\n",
        "print(f\"Total rows in merged file: {len(merged_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print the merged file size\n",
        "mem_usage_mb = merged_df.memory_usage(deep=True).sum() / (1024**2)\n",
        "print(f\"Total memory size: {mem_usage_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the final merged file saved to the universal /content folder\n",
        "df_merged = pd.read_csv(\"/content/merged_submissions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: View overall stats as a DataFrame\n",
        "row_count = len(df_merged)\n",
        "mem_usage_mb = df_merged.memory_usage(deep=True).sum() / (1024**2)\n",
        "\n",
        "stats_data = {\n",
        "    'RowCount': [row_count],\n",
        "    'Size_MB': [mem_usage_mb]\n",
        "}\n",
        "stats_df = pd.DataFrame(stats_data)\n",
        "stats_df['Size_MB'] = stats_df['Size_MB'].round(4)\n",
        "print(\"DataFrame with overall statistics:\")\n",
        "stats_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Frequency overview of data types\n",
        "print(\"--- Frequency overview of data types (merged_df) ---\")\n",
        "data_type_frequency = df_merged.dtypes.value_counts()\n",
        "print(data_type_frequency)"
      ]
    }
  ]
}
