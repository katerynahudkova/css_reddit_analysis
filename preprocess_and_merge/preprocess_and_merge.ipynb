{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing all comment files in the directory: removing NaNs and duplicates, deleted comments, normalizing author names, converting all texts to lower case, and merging them into one csv file with pre-processed submissions.\n",
        "\n",
        "This can be quite computationally expensive, so we advice using cloud services for the task."
      ],
      "metadata": {
        "id": "s8PyuX1pmpIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buLVBOJd7wj8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comments_dir = \"/PATH/comments\"\n",
        "submissions = \"/PATH/merged_submissions.csv\"\n",
        "\n",
        "output_path = os.path.join(\"/OUTPUT_PATH\", \"merged_dataset.csv\")\n",
        "\n",
        "files = glob.glob(os.path.join(comments_dir, \"*.csv\"))\n",
        "\n",
        "cleaned_dfs = []"
      ],
      "metadata": {
        "id": "S3Pdx7D076sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_comments(df):\n",
        "    df = df.dropna(subset=[\"body\"]).copy()\n",
        "    df = df.drop_duplicates(subset=[\"author\", \"body\", \"created\"])\n",
        "\n",
        "    df[\"body\"] = (\n",
        "        df[\"body\"]\n",
        "        .astype(str)\n",
        "        .str.lower()\n",
        "        .str.strip()\n",
        "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        .str.replace(r\"[^a-zA-Z0-9\\s'.,!?-]\", \"\", regex=True)\n",
        "    )\n",
        "\n",
        "    df = df[~df[\"body\"].isin([\"removed\", \"deleted\"])]\n",
        "\n",
        "    df[\"author\"] = df[\"author\"].astype(str).str.strip().str.replace(r\"^u/\", \"\", regex=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "evoiZlDKL3ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files"
      ],
      "metadata": {
        "id": "KjK61_Of9kJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in files:\n",
        "    print(f\"Cleaning {file} ...\")\n",
        "    df = pd.read_csv(file)\n",
        "    cleaned = clean_comments(df)\n",
        "    cleaned_dfs.append(cleaned)"
      ],
      "metadata": {
        "id": "Qvh9d_VVAltI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_comments = pd.concat(cleaned_dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "zGF8AlgCBhnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submissions_df = pd.read_csv(submissions)\n",
        "submissions_df = submissions_df.rename(columns={\"text\": \"body\"}) # normalizing columns for consistency"
      ],
      "metadata": {
        "id": "BGthcVTSR6Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final = pd.concat([merged_comments, submissions_df], ignore_index=True)\n",
        "merged_final = merged_final.drop_duplicates(subset=[\"author\", \"body\", \"created\"])"
      ],
      "metadata": {
        "id": "JlwXlDdwSH1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final[\"created\"] = pd.to_datetime(merged_final[\"created\"], errors=\"coerce\")\n",
        "merged_final = merged_final.sort_values(by=\"created\", ascending=True).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "jkp588ziUEcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_final)"
      ],
      "metadata": {
        "id": "QqrEn4sZUHgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final.dtypes.value_counts()"
      ],
      "metadata": {
        "id": "dZfqqucRULMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_posts = merged_final[\"title\"].notna().sum()\n",
        "num_comments = merged_final[\"title\"].isna().sum()\n",
        "\n",
        "print(f\"Comments: {num_comments:,}\")\n",
        "print(f\"Submissions: {num_posts:,}\")"
      ],
      "metadata": {
        "id": "RirQmWzJU8Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final.info()"
      ],
      "metadata": {
        "id": "ahKlmDJt7k4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final.head()"
      ],
      "metadata": {
        "id": "uuMplyD2Bxp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_final.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "1JcMiAyUBvNk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}